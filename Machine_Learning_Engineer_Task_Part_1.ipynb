{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_Engineer_Task_Part_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "RRvDJOoVHiBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNsjpx6PHeQG",
        "outputId": "c588e002-7e9e-4d9a-b197-401278d6c0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy import tokenizer\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "Cwc-BnzPN6e5"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "Cm9zIEPTIBI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(path):\n",
        "    text_file = open(path, \"r\")\n",
        "    data = text_file.read()\n",
        "    text_file.close()\n",
        "    return data\n",
        "\n",
        "def write_file(path, data):\n",
        "    text_file = open(path, \"w\")\n",
        "    n = text_file.write(data)\n",
        "    text_file.close()\n",
        "\n",
        "def clean_data(data):\n",
        "    soup = BeautifulSoup(data, \"html.parser\")\n",
        "    for txt in soup(['style', 'script']):\n",
        "        txt.decompose()\n",
        "    return ' '.join(soup.stripped_strings)\n",
        "\n",
        "def get_sentences(data):\n",
        "  # parsing the html file\n",
        "  htmlParse = BeautifulSoup(data, 'html.parser')\n",
        "  sentences = []\n",
        "    \n",
        "  # getting all the paragraphs\n",
        "  for para in htmlParse.find_all(\"p\"):\n",
        "      para_txt = para.get_text()\n",
        "      sentences_para = nltk.sent_tokenize(para_txt)\n",
        "      for sent in sentences_para:\n",
        "        sentences.append(sent.replace('\\n', ' '))\n",
        "\n",
        "  return sentences\n",
        "\n",
        "def get_labels(data):\n",
        "  htmlParse = BeautifulSoup(data, 'html.parser')\n",
        "  labels = {'locations':[], 'organizations':[], 'persons':[]}\n",
        "\n",
        "  for i in htmlParse.findAll(attrs={'type' : 'LOCATION'}):\n",
        "      labels['locations'].append(i.text.replace('\\n', ' '))\n",
        "\n",
        "  for i in htmlParse.findAll(attrs={'type' : 'ORGANIZATION'}):\n",
        "      labels['organizations'].append(i.text.replace('\\n', ' '))\n",
        "\n",
        "  for i in htmlParse.findAll(attrs={'type' : 'PERSON'}):\n",
        "      labels['persons'].append(i.text.replace('\\n', ' '))\n",
        "  print(\"Entities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(labels['locations']), len(labels['organizations']), len(labels['persons'])))\n",
        "\n",
        "  print(\"\\nUnique words entities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(set(labels['locations'])), len(set(labels['organizations'])), len(set(labels['persons']))))\n",
        "\n",
        "  return labels"
      ],
      "metadata": {
        "id": "hCtFKG7AH9LI"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(sentences[1])\n",
        "tagged = nltk.pos_tag(words)\n",
        "namedEnt = nltk.ne_chunk(tagged, binary=False)"
      ],
      "metadata": {
        "id": "PLM64XFwdMv9"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "id": "CYEJTyougJYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in namedEnt:\n",
        "      if hasattr(chunk, 'label'):\n",
        "         print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "metadata": {
        "id": "LAuZt_75ez1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_read = r'/content/news_sample_ner.txt'\n",
        "path_write = r'/content/news_sample_ner_cleaned.txt'"
      ],
      "metadata": {
        "id": "dIHg78_KIfIB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_file(path_read)"
      ],
      "metadata": {
        "id": "g319v0R7JFyg"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = clean_data(data)\n",
        "print(cleaned_data)\n",
        "write_file(path_write, cleaned_data)"
      ],
      "metadata": {
        "id": "XdGkCzDRIrTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = get_sentences(data)"
      ],
      "metadata": {
        "id": "Flmford4Ivam"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = get_labels(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMfSqul6K72E",
        "outputId": "11c8ee07-33d2-4c41-e4b1-03670b4aed08"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities distribution:\n",
            "Locations: 167\n",
            "Organizations: 173\n",
            "Persons: 93\n",
            "\n",
            "Unique words entities distribution:\n",
            "Locations: 81\n",
            "Organizations: 70\n",
            "Persons: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "rY0n_eeOODC2"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_spacy = {'locations':[], 'organizations':[], 'persons':[]} "
      ],
      "metadata": {
        "id": "CLGweNbzSnCs"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  doc = nlp(sentence)\n",
        "  ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
        "  for ent in ents:\n",
        "    print(ent[-1])\n",
        "    if ent[-1] == 'LOC' or ent[-1] == 'GPE':\n",
        "      preds_spacy['locations'].append(ent[0])\n",
        "    elif ent[-1] == 'ORG':\n",
        "      preds_spacy['organizations'].append(ent[0])\n",
        "    elif ent[-1] == 'PERSON':\n",
        "      preds_spacy['persons'].append(ent[0])"
      ],
      "metadata": {
        "id": "2a8PGmS3SOCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_spacy['persons']"
      ],
      "metadata": {
        "id": "HMFPa559T2_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sentences[2])"
      ],
      "metadata": {
        "id": "3yE2fE77OEO5"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]"
      ],
      "metadata": {
        "id": "eUXw8DqtOody"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq-1ScwcOyTE",
        "outputId": "3c9ff57f-f989-4be4-c9d0-3eb1e6c8b5be"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Branson', 0, 7, 'GPE'),\n",
              " ('Lindstrand', 12, 22, 'GPE'),\n",
              " ('first', 74, 79, 'ORDINAL'),\n",
              " ('the Atlantic Ocean', 121, 139, 'LOC'),\n",
              " ('1987', 144, 148, 'DATE'),\n",
              " ('Pacific', 158, 165, 'LOC'),\n",
              " ('1991', 170, 174, 'DATE')]"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ents[0][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zhYbxrlPSWh5",
        "outputId": "9bd7574f-1eb6-4843-872c-a41175840d67"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NORP'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels['locations']"
      ],
      "metadata": {
        "id": "1dbwz5X6O7wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9KN7--wFRtoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "2mz4VpwBe2G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_precision(preds, labels, entity):\n",
        "  cnt = 0\n",
        "  for pred in preds[entity]:\n",
        "    if pred in labels[entity]:\n",
        "      cnt+=1\n",
        "  #print(cnt)\n",
        "  return cnt / len(preds[entity])"
      ],
      "metadata": {
        "id": "Yqp7SPNJaC5B"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_recall(preds, labels, entity):\n",
        "  cnt = 0\n",
        "  for label in labels[entity]:\n",
        "    if label in preds[entity]:\n",
        "      cnt+=1\n",
        "  #print(cnt)\n",
        "  return cnt / len(labels[entity])"
      ],
      "metadata": {
        "id": "gm3AnH9xbPxw"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_f1_score(precision, recall):\n",
        "  return (2 * precision * recall) / (precision + recall)"
      ],
      "metadata": {
        "id": "U8Z8akPMb9QB"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_metrics(preds, labels):\n",
        "\n",
        "  person_precision = calc_precision(preds, labels, 'persons')\n",
        "  #print(person_precision)\n",
        "  locations_precision = calc_precision(preds, labels, 'locations')\n",
        "  #print(locations_precision)\n",
        "  organizations_precision = calc_precision(preds, labels, 'organizations')\n",
        "  #print(organizations_precision)\n",
        "\n",
        "  person_recall = calc_recall(preds, labels, 'persons')\n",
        "  #print(person_recall)\n",
        "  locations_recall = calc_recall(preds, labels, 'locations')\n",
        "  #print(locations_recall)\n",
        "  organizations_recall = calc_recall(preds, labels, 'organizations')\n",
        "  #print(organizations_recall)\n",
        "\n",
        "  person_f1_score = calc_f1_score(person_precision, person_recall)\n",
        "  #print(person_f1_score)\n",
        "  locations_f1_score = calc_f1_score(locations_precision, locations_recall)\n",
        "  #print(locations_f1_score)\n",
        "  organizations_f1_score = calc_f1_score(organizations_precision, organizations_recall)\n",
        "  #print(organizations_f1_score)\n",
        "\n",
        "  #print('\\n')\n",
        "  print('Persons:')\n",
        "  print('Precision: {} \\t Recall: {} \\t F1-score: {}'.format(person_precision, person_recall, person_f1_score))\n",
        "  print('\\n')\n",
        "  print('Locations:')\n",
        "  print('Precision: {} \\t Recall: {} \\t F1-score: {}'.format(locations_precision, locations_recall, locations_f1_score))\n",
        "  print('\\n')\n",
        "  print('Organizations:')\n",
        "  print('Precision: {} \\t Recall: {} \\t F1-score: {}'.format(organizations_precision, organizations_recall, organizations_f1_score))"
      ],
      "metadata": {
        "id": "FXKfD1FSV-ce"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER with statistical models"
      ],
      "metadata": {
        "id": "ep0zva-dUWr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_spacy(sentences):\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  preds_spacy = {'locations':[], 'organizations':[], 'persons':[]}\n",
        "\n",
        "  for sentence in sentences:\n",
        "    doc = nlp(sentence)\n",
        "    ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
        "    for ent in ents:\n",
        "      #print(ent[-1])\n",
        "      if ent[-1] == 'LOC' or ent[-1] == 'GPE':\n",
        "        preds_spacy['locations'].append(ent[0])\n",
        "      elif ent[-1] == 'ORG':\n",
        "        preds_spacy['organizations'].append(ent[0])\n",
        "      elif ent[-1] == 'PERSON':\n",
        "        preds_spacy['persons'].append(ent[0])\n",
        "  \n",
        "  print(\"Entities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(preds_spacy['locations']), len(preds_spacy['organizations']), len(preds_spacy['persons'])))\n",
        "\n",
        "  print(\"\\nUnique words entities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(set(preds_spacy['locations'])), len(set(preds_spacy['organizations'])), len(set(preds_spacy['persons']))))\n",
        "  \n",
        "  return preds_spacy"
      ],
      "metadata": {
        "id": "VbcaRUilUegk"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_nltk(sentences):\n",
        "  preds_nltk = {'locations':[], 'organizations':[], 'persons':[]}\n",
        "\n",
        "  for sentence in sentences:\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "    chunks = nltk.ne_chunk(tagged, binary=False)\n",
        "\n",
        "    for chunk in chunks:\n",
        "      if hasattr(chunk,'label'):\n",
        "          #print(chunk.label())\n",
        "          if chunk.label() == 'LOCATION' or chunk.label() == 'GPE':\n",
        "            preds_nltk['locations'].append(' '.join(c[0] for c in chunk))\n",
        "          elif chunk.label() == 'ORGANIZATION':\n",
        "            preds_nltk['organizations'].append(' '.join(c[0] for c in chunk))\n",
        "          elif chunk.label() == 'PERSON':\n",
        "            preds_nltk['persons'].append(' '.join(c[0] for c in chunk))\n",
        "  \n",
        "  print(\"Entities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(preds_nltk['locations']), len(preds_nltk['organizations']), len(preds_nltk['persons'])))\n",
        "\n",
        "  print(\"\\nUnique words entities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(set(preds_nltk['locations'])), len(set(preds_nltk['organizations'])), len(set(preds_nltk['persons']))))\n",
        "  \n",
        "  return preds_nltk"
      ],
      "metadata": {
        "id": "lJuI_TSun2NM"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_spacy = predict_spacy(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMFS9kXDU1Yh",
        "outputId": "0289e706-ca2a-4dc3-91c2-1f1191c57359"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities distribution:\n",
            "Locations: 1181\n",
            "Organizations: 1466\n",
            "Persons: 580\n",
            "\n",
            "Unique words entities distribution:\n",
            "Locations: 69\n",
            "Organizations: 91\n",
            "Persons: 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_metrics(preds_spacy, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuWHzuEHZsEC",
        "outputId": "c6551a4a-84ba-4027-b3a9-687d062947d3"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persons:\n",
            "Precision: 0.7724137931034483 \t Recall: 0.5698924731182796 \t F1-score: 0.6558753660018783\n",
            "\n",
            "\n",
            "Locations:\n",
            "Precision: 0.8196443691786621 \t Recall: 0.8023952095808383 \t F1-score: 0.8109280735206776\n",
            "\n",
            "\n",
            "Organizations:\n",
            "Precision: 0.5381991814461119 \t Recall: 0.6763005780346821 \t F1-score: 0.5993980890789206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_nltk = predict_nltk(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUzwRJRYiTkS",
        "outputId": "891dcc0d-d585-402d-8f3a-f67563835055"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities distribution:\n",
            "Locations: 1726\n",
            "Organizations: 1079\n",
            "Persons: 1015\n",
            "\n",
            "Unique words entities distribution:\n",
            "Locations: 93\n",
            "Organizations: 73\n",
            "Persons: 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_metrics(preds_nltk, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1NcYnD3kZCk",
        "outputId": "77977e4b-da2f-4196-8a3e-e6dc6b530afe"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persons:\n",
            "Precision: 0.6216748768472906 \t Recall: 0.7096774193548387 \t F1-score: 0.6627676589243513\n",
            "\n",
            "\n",
            "Locations:\n",
            "Precision: 0.492468134414832 \t Recall: 0.7844311377245509 \t F1-score: 0.6050709674614733\n",
            "\n",
            "\n",
            "Organizations:\n",
            "Precision: 0.46617238183503246 \t Recall: 0.5606936416184971 \t F1-score: 0.5090827516407382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bwLgI3rioe6Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}