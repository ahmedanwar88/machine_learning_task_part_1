{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_Engineer_Task_Part_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloads and Installations"
      ],
      "metadata": {
        "id": "4nDAygXRhyKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "FCrq1fcer3me",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61f5bc8-efbb-44be-9b9e-eb70406dc80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy[transformers,cuda111]"
      ],
      "metadata": {
        "id": "rOTXhyIxsHy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7900e7d5-2a3d-4942-d2d8-a6a270fcb599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy[cuda111,transformers] in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (0.6.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (0.9.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (4.64.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (0.4.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (1.0.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (8.0.17)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (0.7.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (2.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (4.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (1.0.7)\n",
            "Requirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (1.1.6)\n",
            "Requirement already satisfied: cupy-cuda111<11.0.0,>=5.0.0b4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (10.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy[cuda111,transformers]) (3.8.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda111<11.0.0,>=5.0.0b4->spacy[cuda111,transformers]) (0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[cuda111,transformers]) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[cuda111,transformers]) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda111,transformers]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda111,transformers]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda111,transformers]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda111,transformers]) (1.24.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (1.11.0+cu113)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (0.8.5)\n",
            "Requirement already satisfied: transformers<4.20.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (4.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (4.11.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (0.8.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (0.12.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy[cuda111,transformers]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[cuda111,transformers]) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "8OAE-tK1q71k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657ee619-18d3-4910-a1ff-6735fe618a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-trf==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.3.0/en_core_web_trf-3.3.0-py3-none-any.whl (460.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 460.3 MB 21 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.3.0) (1.1.6)\n",
            "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (0.7.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (0.8.5)\n",
            "Requirement already satisfied: transformers<4.20.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (4.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (0.8.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (4.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (3.7.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install spacy-transformers"
      ],
      "metadata": {
        "id": "MAfQ7TNzs8Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check that the transformers package is installed\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_trf')"
      ],
      "metadata": {
        "id": "frfPHyKMiFOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/software/stanford-ner-4.2.0.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWoUq8XM8H7f",
        "outputId": "8df5d0aa-046e-4c64-aa22-09ad989d2c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-30 12:31:26--  https://nlp.stanford.edu/software/stanford-ner-4.2.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-ner-4.2.0.zip [following]\n",
            "--2022-06-30 12:31:27--  https://downloads.cs.stanford.edu/nlp/software/stanford-ner-4.2.0.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180437064 (172M) [application/zip]\n",
            "Saving to: ‘stanford-ner-4.2.0.zip’\n",
            "\n",
            "stanford-ner-4.2.0. 100%[===================>] 172.08M  5.08MB/s    in 31s     \n",
            "\n",
            "2022-06-30 12:31:58 (5.54 MB/s) - ‘stanford-ner-4.2.0.zip’ saved [180437064/180437064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/stanford-ner-4.2.0.zip"
      ],
      "metadata": {
        "id": "uVp17x-I8I2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22939ff7-d1e9-46b9-c667-e1b4eabcee34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/stanford-ner-4.2.0.zip\n",
            "   creating: stanford-ner-2020-11-17/\n",
            "   creating: stanford-ner-2020-11-17/lib/\n",
            "  inflating: stanford-ner-2020-11-17/lib/jollyday-0.4.9.jar  \n",
            "  inflating: stanford-ner-2020-11-17/lib/stanford-ner-resources.jar  \n",
            "  inflating: stanford-ner-2020-11-17/lib/joda-time.jar  \n",
            "  inflating: stanford-ner-2020-11-17/stanford-ner-4.2.0.jar  \n",
            "  inflating: stanford-ner-2020-11-17/NERDemo.java  \n",
            "  inflating: stanford-ner-2020-11-17/LICENSE.txt  \n",
            "  inflating: stanford-ner-2020-11-17/sample-conll-file.txt  \n",
            "  inflating: stanford-ner-2020-11-17/stanford-ner-4.2.0-javadoc.jar  \n",
            "  inflating: stanford-ner-2020-11-17/stanford-ner-4.2.0-sources.jar  \n",
            "  inflating: stanford-ner-2020-11-17/stanford-ner.jar  \n",
            "  inflating: stanford-ner-2020-11-17/sample.txt  \n",
            "  inflating: stanford-ner-2020-11-17/build.xml  \n",
            "  inflating: stanford-ner-2020-11-17/ner-gui.bat  \n",
            "  inflating: stanford-ner-2020-11-17/sample-w-time.txt  \n",
            "  inflating: stanford-ner-2020-11-17/ner-gui.command  \n",
            "   creating: stanford-ner-2020-11-17/classifiers/\n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.muc.7class.distsim.prop  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.conll.4class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/example.serialized.ncc.prop  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.muc.7class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.prop  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/example.serialized.ncc.ncc.ser.gz  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.conll.4class.distsim.prop  \n",
            "  inflating: stanford-ner-2020-11-17/ner-gui.sh  \n",
            "  inflating: stanford-ner-2020-11-17/sample.ner.txt  \n",
            "  inflating: stanford-ner-2020-11-17/ner.bat  \n",
            "  inflating: stanford-ner-2020-11-17/README.txt  \n",
            "  inflating: stanford-ner-2020-11-17/ner.sh  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ],
      "metadata": {
        "id": "ojKBUL0YhRyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class configs:\n",
        "  data_path = '/content/news_sample_ner.txt'\n",
        "  cleaned_data_path = '/content/news_sample_ner_cleaned.txt'\n",
        "  stanford_ner_model_path = '/content/stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
        "  stanford_ner_jar_path = '/content/stanford-ner-2020-11-17/stanford-ner.jar'\n"
      ],
      "metadata": {
        "id": "iJ_icFT3hUXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "RRvDJOoVHiBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNsjpx6PHeQG",
        "outputId": "11a2e509-62c6-47cb-8353-17190b728538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tag import StanfordNERTagger\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy import tokenizer\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby"
      ],
      "metadata": {
        "id": "Cwc-BnzPN6e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning and Preparation"
      ],
      "metadata": {
        "id": "Cm9zIEPTIBI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(path):\n",
        "    \"\"\"Reads a text file from the disk.\n",
        "\n",
        "    Args:\n",
        "      path: Path to the file\n",
        "\n",
        "    Returns:\n",
        "      data: Text of the file\n",
        "    \"\"\"\n",
        "    text_file = open(path, \"r\")\n",
        "    data = text_file.read()\n",
        "    text_file.close()\n",
        "    return data\n",
        "\n",
        "def write_file(path, data):\n",
        "    \"\"\"Writes a file to the disk.\n",
        "\n",
        "      Args:\n",
        "        path: path of writing the file\n",
        "        data: Text to be written\n",
        "    \"\"\"\n",
        "    text_file = open(path, \"w\")\n",
        "    n = text_file.write(data)\n",
        "    text_file.close()\n",
        "\n",
        "def clean_data(data):\n",
        "    \"\"\"Cleans text from HTML tags.\n",
        "\n",
        "    Args:\n",
        "      data: Text to be cleaned\n",
        "\n",
        "    Returns:\n",
        "      Cleaned text\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(data, \"html.parser\")\n",
        "    for txt in soup(['style', 'script']):\n",
        "        txt.decompose()\n",
        "    return ' '.join(soup.stripped_strings)\n",
        "\n",
        "def get_sentences(data):\n",
        "  \"\"\"Tokenizes the data into sentences.\n",
        "\n",
        "    Args:\n",
        "      data: Text to be tokenized\n",
        "\n",
        "    Returns:\n",
        "      sentences: Tokenized sentences\n",
        "  \"\"\"\n",
        "  htmlParse = BeautifulSoup(data, 'html.parser')\n",
        "  sentences = []\n",
        "    \n",
        "  for para in htmlParse.find_all(\"p\"):\n",
        "      para_txt = para.get_text()\n",
        "      sentences_para = nltk.sent_tokenize(para_txt)\n",
        "      for sent in sentences_para:\n",
        "        sentences.append(sent.replace('\\n', ' '))\n",
        "\n",
        "  return sentences\n",
        "\n",
        "def get_labels(data):\n",
        "  \"\"\"Gets the named entities from the data.\n",
        "\n",
        "    Args:\n",
        "      data: Text to get the named entities from\n",
        "\n",
        "    Returns:\n",
        "      labels: A dictionary where keys are: locations, organizations and persons, and values are the entities corresponding to each key\n",
        "  \"\"\"\n",
        "  htmlParse = BeautifulSoup(data, 'html.parser')\n",
        "  labels = {'locations':[], 'organizations':[], 'persons':[]}\n",
        "\n",
        "  for i in htmlParse.findAll(attrs={'type' : 'LOCATION'}):\n",
        "      labels['locations'].append(i.text.replace('\\n', ' '))\n",
        "\n",
        "  for i in htmlParse.findAll(attrs={'type' : 'ORGANIZATION'}):\n",
        "      labels['organizations'].append(i.text.replace('\\n', ' '))\n",
        "\n",
        "  for i in htmlParse.findAll(attrs={'type' : 'PERSON'}):\n",
        "      labels['persons'].append(i.text.replace('\\n', ' '))\n",
        "  print(\"Entities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(labels['locations']), len(labels['organizations']), len(labels['persons'])))\n",
        "\n",
        "  print(\"\\nUnique words entities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(set(labels['locations'])), len(set(labels['organizations'])), len(set(labels['persons']))))\n",
        "\n",
        "  return labels"
      ],
      "metadata": {
        "id": "hCtFKG7AH9LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_file(configs.data_path)"
      ],
      "metadata": {
        "id": "g319v0R7JFyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = clean_data(data)\n",
        "write_file(configs.cleaned_data_path, cleaned_data)"
      ],
      "metadata": {
        "id": "XdGkCzDRIrTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = get_sentences(data)"
      ],
      "metadata": {
        "id": "Flmford4Ivam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = get_labels(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RgLvhadjp1d",
        "outputId": "349806fb-0455-47ab-ea65-bfd10b53d68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities distribution:\n",
            "Locations: 167\n",
            "Organizations: 173\n",
            "Persons: 93\n",
            "\n",
            "Unique words entities distribution:\n",
            "Locations: 81\n",
            "Organizations: 70\n",
            "Persons: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics"
      ],
      "metadata": {
        "id": "2mz4VpwBe2G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_precision(preds, labels, entity):\n",
        "  \"\"\"Calculates precision.\n",
        "\n",
        "    Args:\n",
        "      preds: A prediction dictionary where keys are locations, organizations and persons, and values are the predicted entities for each key\n",
        "      labels: A ground truth dictionary where keys are locations, organizations and persons, and values are the ground truth entities for each key\n",
        "      entity: The entity to calculate precision for (locations or organizations or persons)\n",
        "\n",
        "    Returns:\n",
        "      The calculated precision for the entity\n",
        "  \"\"\"\n",
        "  cnt = 0\n",
        "  for pred in preds[entity]:\n",
        "    if pred in labels[entity]:\n",
        "      cnt+=1\n",
        "  return cnt / len(preds[entity])"
      ],
      "metadata": {
        "id": "Yqp7SPNJaC5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_recall(preds, labels, entity):\n",
        "  \"\"\"Calculates recall.\n",
        "\n",
        "    Args:\n",
        "      preds: A prediction dictionary where keys are locations, organizations and persons, and values are the predicted entities for each key\n",
        "      labels: A ground truth dictionary where keys are locations, organizations and persons, and values are the ground truth entities for each key\n",
        "      entity: The entity to calculate recall for (locations or organizations or persons)\n",
        "\n",
        "    Returns:\n",
        "      The calculated recall for the entity\n",
        "  \"\"\"\n",
        "  cnt = 0\n",
        "  for label in labels[entity]:\n",
        "    if label in preds[entity]:\n",
        "      cnt+=1\n",
        "  return cnt / len(labels[entity])"
      ],
      "metadata": {
        "id": "gm3AnH9xbPxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_f1_score(precision, recall):\n",
        "  \"\"\"Calculates F1-score.\n",
        "\n",
        "    Args:\n",
        "      precision: The calculated precision\n",
        "      recall: The calculated recall\n",
        "\n",
        "    Returns:\n",
        "      The calculated F1-score\n",
        "  \"\"\"\n",
        "  return (2 * precision * recall) / (precision + recall)"
      ],
      "metadata": {
        "id": "U8Z8akPMb9QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_metrics(preds, labels):\n",
        "  \"\"\"Reports precision, recall and F1-score.\n",
        "\n",
        "    Args:\n",
        "      preds: A prediction dictionary where keys are locations, organizations and persons, and values are the predicted entities for each key\n",
        "      labels: A ground truth dictionary where keys are locations, organizations and persons, and values are the ground truth entities for each key\n",
        "  \"\"\"\n",
        "\n",
        "  person_precision = calc_precision(preds, labels, 'persons')\n",
        "  locations_precision = calc_precision(preds, labels, 'locations')\n",
        "  organizations_precision = calc_precision(preds, labels, 'organizations')\n",
        "\n",
        "  person_recall = calc_recall(preds, labels, 'persons')\n",
        "  locations_recall = calc_recall(preds, labels, 'locations')\n",
        "  organizations_recall = calc_recall(preds, labels, 'organizations')\n",
        "\n",
        "  person_f1_score = calc_f1_score(person_precision, person_recall)\n",
        "  locations_f1_score = calc_f1_score(locations_precision, locations_recall)\n",
        "  organizations_f1_score = calc_f1_score(organizations_precision, organizations_recall)\n",
        "\n",
        "  print('***** Persons *****')\n",
        "  print('Precision: {:.3f} \\t Recall: {:.3f} \\t F1-score: {:.3f}'.format(person_precision, person_recall, person_f1_score))\n",
        "  print('\\n')\n",
        "  print('***** Locations *****')\n",
        "  print('Precision: {:.3f} \\t Recall: {:.3f} \\t F1-score: {:.3f}'.format(locations_precision, locations_recall, locations_f1_score))\n",
        "  print('\\n')\n",
        "  print('***** Organizations *****')\n",
        "  print('Precision: {:.3f} \\t Recall: {:.3f} \\t F1-score: {:.3f}'.format(organizations_precision, organizations_recall, organizations_f1_score))"
      ],
      "metadata": {
        "id": "FXKfD1FSV-ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Functions"
      ],
      "metadata": {
        "id": "mECijbbpevQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_nltk_stanford_ner(sentences, model_path, jar_path):\n",
        "  \"\"\"Predicts named entities from a list of sentences based on Stanford NER model.\n",
        "\n",
        "    Args:\n",
        "      sentences: A list of tokenized sentences\n",
        "      model_path: Path to model\n",
        "      jar_path: Path to jar file\n",
        "\n",
        "    Returns:\n",
        "      preds_ntlk: A prediction dictionary where keys are locations, organizations and persons, and values are the predicted entities for each key\n",
        "  \"\"\"\n",
        "  model = model_path\n",
        "  jar = jar_path\n",
        "  #model = '/content/stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
        "  #jar = '/content/stanford-ner-2020-11-17/stanford-ner.jar'\n",
        "  st = StanfordNERTagger(model, jar,encoding='utf-8')\n",
        "\n",
        "  preds_ntlk = {'locations':[], 'organizations':[], 'persons':[]}\n",
        "\n",
        "  for sentence in tqdm(sentences):\n",
        "    tokenized_text = nltk.word_tokenize(sentence)\n",
        "    classified_text = st.tag(tokenized_text)\n",
        "\n",
        "    entities = []\n",
        "    labels = []\n",
        "    \n",
        "    for tag, chunk in groupby(classified_text, lambda x:x[1]):\n",
        "        if tag != \"O\":\n",
        "            entities.append(' '.join(w for w, t in chunk))\n",
        "            labels.append(tag)\n",
        "    \n",
        "\n",
        "    for entity, label in zip(entities, labels):\n",
        "      if label == 'LOCATION':\n",
        "        preds_ntlk['locations'].append(entity)\n",
        "      elif label == 'ORGANIZATION':\n",
        "        preds_ntlk['organizations'].append(entity)\n",
        "      elif label == 'PERSON':\n",
        "        preds_ntlk['persons'].append(entity)\n",
        "  \n",
        "  print(\"\\nEntities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(preds_ntlk['locations']), len(preds_ntlk['organizations']), len(preds_ntlk['persons'])))\n",
        "\n",
        "  print(\"\\nUnique words entities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(set(preds_ntlk['locations'])), len(set(preds_ntlk['organizations'])), len(set(preds_ntlk['persons']))))\n",
        "  \n",
        "  return preds_ntlk"
      ],
      "metadata": {
        "id": "_QSQHeW7fWyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_nltk(sentences):\n",
        "  \"\"\"Predicts named entities from a list of sentences based on NLTK NER model.\n",
        "\n",
        "    Args:\n",
        "      sentences: A list of tokenized sentences\n",
        "\n",
        "    Returns:\n",
        "      preds_ntlk: A prediction dictionary where keys are locations, organizations and persons, and values are the predicted entities for each key\n",
        "  \"\"\"\n",
        "  preds_nltk = {'locations':[], 'organizations':[], 'persons':[]}\n",
        "\n",
        "  for sentence in tqdm(sentences):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "    chunks = nltk.ne_chunk(tagged, binary=False)\n",
        "\n",
        "    for chunk in chunks:\n",
        "      if hasattr(chunk,'label'):\n",
        "          if chunk.label() == 'LOCATION' or chunk.label() == 'GPE':\n",
        "            preds_nltk['locations'].append(' '.join(c[0] for c in chunk))\n",
        "          elif chunk.label() == 'ORGANIZATION':\n",
        "            preds_nltk['organizations'].append(' '.join(c[0] for c in chunk))\n",
        "          elif chunk.label() == 'PERSON':\n",
        "            preds_nltk['persons'].append(' '.join(c[0] for c in chunk))\n",
        "  \n",
        "  print(\"\\nEntities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(preds_nltk['locations']), len(preds_nltk['organizations']), len(preds_nltk['persons'])))\n",
        "\n",
        "  print(\"\\nUnique words entities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(set(preds_nltk['locations'])), len(set(preds_nltk['organizations'])), len(set(preds_nltk['persons']))))\n",
        "  \n",
        "  return preds_nltk"
      ],
      "metadata": {
        "id": "CivK5XNIfWyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_spacy(sentences, transformer_based = 0):\n",
        "  \"\"\"Predicts named entities from a list of sentences based on Spacy NER model.\n",
        "\n",
        "    Args:\n",
        "      sentences: A list of tokenized sentences\n",
        "      transformer_based: A flag to choose whether to use the transformer basde model or not\n",
        "      1: Use en_core_web_trf model\n",
        "      0: Use en_core_web_sm model\n",
        "\n",
        "    Returns:\n",
        "      preds_spacy: A prediction dictionary where keys are locations, organizations and persons, and values are the predicted entities for each key\n",
        "  \"\"\"\n",
        "  if transformer_based:\n",
        "    nlp = spacy.load('en_core_web_trf')\n",
        "  else:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    \n",
        "  preds_spacy = {'locations':[], 'organizations':[], 'persons':[]}\n",
        "\n",
        "  for sentence in tqdm(sentences):\n",
        "    doc = nlp(sentence)\n",
        "    ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
        "    for ent in ents:\n",
        "      if ent[-1] == 'LOC' or ent[-1] == 'GPE':\n",
        "        preds_spacy['locations'].append(ent[0])\n",
        "      elif ent[-1] == 'ORG':\n",
        "        preds_spacy['organizations'].append(ent[0])\n",
        "      elif ent[-1] == 'PERSON':\n",
        "        preds_spacy['persons'].append(ent[0])\n",
        "  \n",
        "  print(\"\\nEntities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(preds_spacy['locations']), len(preds_spacy['organizations']), len(preds_spacy['persons'])))\n",
        "\n",
        "  print(\"\\nUnique words entities distribution:\")\n",
        "\n",
        "  print(\"Locations: {}\\nOrganizations: {}\\nPersons: {}\".format(len(set(preds_spacy['locations'])), len(set(preds_spacy['organizations'])), len(set(preds_spacy['persons']))))\n",
        "  \n",
        "  return preds_spacy"
      ],
      "metadata": {
        "id": "-fvmMcvzev-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER with Statistical Models"
      ],
      "metadata": {
        "id": "ep0zva-dUWr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stanford NER Model"
      ],
      "metadata": {
        "id": "fHyGGhQoIey-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_nltk_stanford_ner = predict_nltk_stanford_ner(sentences, configs.stanford_ner_model_path, configs.stanford_ner_jar_path)"
      ],
      "metadata": {
        "id": "4HATZCHvkafK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d90221f-4342-4673-9691-373f7938d9ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2578/2578 [2:13:45<00:00,  3.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entities distribution:\n",
            "Locations: 1233\n",
            "Organizations: 1231\n",
            "Persons: 795\n",
            "\n",
            "Unique words entities distribution:\n",
            "Locations: 73\n",
            "Organizations: 60\n",
            "Persons: 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_metrics(preds_nltk_stanford_ner, labels)"
      ],
      "metadata": {
        "id": "z_EykwjZkarB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af49629e-c168-4b36-815d-ec3678d0cfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Persons *****\n",
            "Precision: 0.964 \t Recall: 0.839 \t F1-score: 0.897\n",
            "\n",
            "\n",
            "***** Locations *****\n",
            "Precision: 0.972 \t Recall: 0.898 \t F1-score: 0.934\n",
            "\n",
            "\n",
            "***** Organizations *****\n",
            "Precision: 0.944 \t Recall: 0.792 \t F1-score: 0.861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy NER Model"
      ],
      "metadata": {
        "id": "vN7zp587IkkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_spacy = predict_spacy(sentences, transformer_based=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiaBJeTefpPO",
        "outputId": "bcb1bf74-f770-4ad5-fc38-4ffb19b10db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2578/2578 [00:26<00:00, 98.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entities distribution:\n",
            "Locations: 1181\n",
            "Organizations: 1466\n",
            "Persons: 580\n",
            "\n",
            "Unique words entities distribution:\n",
            "Locations: 69\n",
            "Organizations: 91\n",
            "Persons: 52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_metrics(preds_spacy, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP0-nmfTfplm",
        "outputId": "7b418f1c-087c-4a00-9306-4dc536567203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Persons *****\n",
            "Precision: 0.772 \t Recall: 0.570 \t F1-score: 0.656\n",
            "\n",
            "\n",
            "***** Locations *****\n",
            "Precision: 0.820 \t Recall: 0.802 \t F1-score: 0.811\n",
            "\n",
            "\n",
            "***** Organizations *****\n",
            "Precision: 0.538 \t Recall: 0.676 \t F1-score: 0.599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK NER Model"
      ],
      "metadata": {
        "id": "RbGNvt9lIoGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_nltk = predict_nltk(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EltWoWiVgDCV",
        "outputId": "6aa4d760-6449-4b87-9350-4d9ebd10de53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2578/2578 [00:27<00:00, 94.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entities distribution:\n",
            "Locations: 1726\n",
            "Organizations: 1079\n",
            "Persons: 1015\n",
            "\n",
            "Unique words entities distribution:\n",
            "Locations: 93\n",
            "Organizations: 73\n",
            "Persons: 79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_metrics(preds_nltk, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu5iH-LtgDRC",
        "outputId": "0bb19eac-d93c-4ae7-fa95-8c3884694d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Persons *****\n",
            "Precision: 0.622 \t Recall: 0.710 \t F1-score: 0.663\n",
            "\n",
            "\n",
            "***** Locations *****\n",
            "Precision: 0.492 \t Recall: 0.784 \t F1-score: 0.605\n",
            "\n",
            "\n",
            "***** Organizations *****\n",
            "Precision: 0.466 \t Recall: 0.561 \t F1-score: 0.509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER with Transformers"
      ],
      "metadata": {
        "id": "qj-TlZKtjc5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy Transformer Based NER Model"
      ],
      "metadata": {
        "id": "9Bl4AQshItGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_spacy_trf = predict_spacy(sentences, transformer_based=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQhxPC_CgnBV",
        "outputId": "c59f0d44-0294-47f8-ec21-b5e03eb7b9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2578 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "100%|██████████| 2578/2578 [07:06<00:00,  6.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entities distribution:\n",
            "Locations: 1264\n",
            "Organizations: 1369\n",
            "Persons: 737\n",
            "\n",
            "Unique words entities distribution:\n",
            "Locations: 74\n",
            "Organizations: 80\n",
            "Persons: 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_metrics(preds_spacy_trf, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vqfmqk4gnM1",
        "outputId": "62bdd748-2b0c-4448-d079-ed73c9819e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Persons *****\n",
            "Precision: 0.984 \t Recall: 0.849 \t F1-score: 0.912\n",
            "\n",
            "\n",
            "***** Locations *****\n",
            "Precision: 0.828 \t Recall: 0.868 \t F1-score: 0.848\n",
            "\n",
            "\n",
            "***** Organizations *****\n",
            "Precision: 0.684 \t Recall: 0.763 \t F1-score: 0.722\n"
          ]
        }
      ]
    }
  ]
}